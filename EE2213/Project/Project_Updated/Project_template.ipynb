{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93a15b08",
   "metadata": {},
   "source": [
    "# EE2213 Project: Multi-Class Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc894ae",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">No additional library imports are permitted.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7df0339",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sympy as sp\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler, OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539146d0",
   "metadata": {},
   "source": [
    "## PART 0: Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de654f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_openml_dataset():\n",
    "\n",
    "    dataset = fetch_openml(name='vehicle', version=1, as_frame=True, parser='auto')\n",
    "    X = dataset.data.values\n",
    "    \n",
    "    target_values = dataset.target.values\n",
    "    unique_targets = np.unique(target_values)\n",
    "    \n",
    "    # Create mapping from string labels to integers\n",
    "    label_to_int = {label: i for i, label in enumerate(unique_targets)}\n",
    "    y = np.array([label_to_int[label] for label in target_values])\n",
    "    \n",
    "    feature_names = dataset.feature_names\n",
    "    \n",
    "    return X, y, feature_names\n",
    "\n",
    "X, y, feature_names = load_openml_dataset()\n",
    "print(f\"Feature shape: {X.shape}, target output shape:{y.shape}\")\n",
    "print(f\"feature names: {feature_names}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ab9a4e",
   "metadata": {},
   "source": [
    "## PART 1: Dataset Partition and One-hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec66350",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_partition_encoding(X, y):\n",
    "    \"\"\"\n",
    "    Input type\n",
    "    :X type: numpy.ndarray of size (number_of_samples, number_of_features)\n",
    "    :y type: numpy.ndarray of size (number_of_samples,)\n",
    "\n",
    "    Return type\n",
    "    :X_train type: numpy.ndarray of size (number_of_training_samples, number_of_features)\n",
    "    :X_val type: numpy.ndarray of size (number_of_validation_samples, number_of_features)\n",
    "    :X_test type: numpy.ndarray of size (number_of_test_samples, number_of_features)\n",
    "    :Ytr_onehot type: numpy.ndarray of size (number_of_training_samples, num_classes)\n",
    "    :Yval_onehot type: numpy.ndarray of size (number_of_validation_samples, num_classes)\n",
    "    :Yts_onehot type: numpy.ndarray of size (number_of_test_samples, num_classes)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # your code goes here\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=665, stratify=y)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=665, stratify=y_temp)\n",
    "    \n",
    "    encoder = OneHotEncoder(sparse_output=False)\n",
    "    Ytr_onehot = encoder.fit_transform(y_train.reshape(-1, 1))\n",
    "    Yval_onehot = encoder.transform(y_val.reshape(-1, 1))\n",
    "    Yts_onehot = encoder.transform(y_test.reshape(-1, 1))\n",
    "    \n",
    "\n",
    "    # return in this order\n",
    "    return X_train, X_val, X_test, Ytr_onehot, Yval_onehot, Yts_onehot\n",
    "\n",
    "X_train, X_val, X_test, Ytr_onehot, Yval_onehot, Yts_onehot = dataset_partition_encoding(X, y)\n",
    "print(f\"Training set shape: {X_train.shape}, {Ytr_onehot.shape}\")\n",
    "print(f\"Validation set shape: {X_val.shape}, {Yval_onehot.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}, {Yts_onehot.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f9bba7",
   "metadata": {},
   "source": [
    "## PART 2: Feature Selection using Pearson Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f793e2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection(X_train, X_val, X_test, feature_names, threshold=0.8):\n",
    "    \"\"\"\n",
    "    Input type\n",
    "    :X_train type: numpy.ndarray of size (number_of_training_samples, number_of_features)\n",
    "    :X_val type: numpy.ndarray of size (number_of_validation_samples, number_of_features)\n",
    "    :X_test type: numpy.ndarray of size (number_of_test_samples, number_of_features)\n",
    "    :feature_names type: list of str\n",
    "    :threshold type: float\n",
    "\n",
    "    Return type\n",
    "    :selected_features type: list of str\n",
    "    :FS_X_train type: numpy.ndarray of size (number_of_training_samples, number_of_selected_features)\n",
    "    :FS_X_val type: numpy.ndarray of size (number_of_validation_samples, number_of_selected_features)\n",
    "    :FS_X_test type: numpy.ndarray of size (number_of_test_samples, number_of_selected_features)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # your code goes here\n",
    "\n",
    "\n",
    "    # return in this order\n",
    "    return selected_features, FS_X_train, FS_X_val, FS_X_test\n",
    "\n",
    "selected_features, FS_X_train, FS_X_val, FS_X_test = feature_selection(X_train, X_val, X_test, feature_names)\n",
    "\n",
    "print(f\"{len(selected_features)} Selected Features: {selected_features}\")\n",
    "print(f\"Training set shape after feature selection: {FS_X_train.shape}, {Ytr_onehot.shape}\")\n",
    "print(f\"Validation set shape after feature selection: {FS_X_val.shape}, {Yval_onehot.shape}\")\n",
    "print(f\"Test set shape after feature selection: {FS_X_test.shape}, {Yts_onehot.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c76d902",
   "metadata": {},
   "source": [
    "## PART 3: Polynomial Feature Transformation and Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b0de36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_for_classification(FS_X_train, FS_X_val, FS_X_test, Ytr_onehot, Yval_onehot, Yts_onehot, max_order=3, lamda=0.001):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        FS_X_train (np.ndarray): Feature matrix for training.\n",
    "        FS_X_val (np.ndarray): Feature matrix for validation.\n",
    "        FS_X_test (np.ndarray): Feature matrix for testing.\n",
    "        Ytr_onehot (np.ndarray): One-hot encoded labels for training.\n",
    "        Yval_onehot (np.ndarray): One-hot encoded labels for validation.\n",
    "        Yts_onehot (np.ndarray): One-hot encoded labels for testing.\n",
    "        max_order (int): Maximum polynomial order to consider.\n",
    "        lamda (float): Regularization strength.\n",
    "\n",
    "    Returns:\n",
    "        acc_train_list (list): Training accuracies for each polynomial order.\n",
    "        acc_val_list (list): Validation accuracies for each polynomial order.\n",
    "        best_order (int): Best polynomial order based on validation accuracy.\n",
    "        acc_test (float): Test accuracy for the best polynomial order.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # your code goes here\n",
    "\n",
    "\n",
    "    # return in this order              \n",
    "    return acc_train_list, acc_val_list, best_order, acc_test\n",
    "\n",
    "acc_train_list, acc_val_list, best_order, acc_test = polynomial_for_classification(FS_X_train, FS_X_val, FS_X_test, Ytr_onehot, Yval_onehot, Yts_onehot)\n",
    "\n",
    "print(f\"Training accuracies: {np.round(acc_train_list,2)}\")\n",
    "print(f\"Validation accuracies: {np.round(acc_val_list,2)}\")\n",
    "print(f\"Best polynomial order: {best_order}\")\n",
    "print(f\"Test accuracy for best order {best_order}: {np.round(acc_test,2)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c8775e",
   "metadata": {},
   "source": [
    "## PART 4: Multinomial Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd90cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLR_select_lr(FS_X_train, FS_X_val, FS_X_test, Ytr_onehot, Yval_onehot, Yts_onehot, lr_list=[0.0001, 0.001, 0.01, 0.1], num_iters=20000):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        FS_X_train (np.ndarray): Feature matrix for training.\n",
    "        FS_X_val (np.ndarray): Feature matrix for validation.\n",
    "        FS_X_test (np.ndarray): Feature matrix for testing.\n",
    "        Ytr_onehot (np.ndarray): One-hot encoded labels for training.\n",
    "        Yval_onehot (np.ndarray): One-hot encoded labels for validation.\n",
    "        Yts_onehot (np.ndarray): One-hot encoded labels for testing.\n",
    "        lr_list (list): List of learning rates to test.\n",
    "        num_iters (int): Number of iterations for training.\n",
    "\n",
    "    Returns:\n",
    "        cost_dict (dict): Dictionary of cost values for each learning rate without input normalization.\n",
    "                          example: cost_dict = {0.0001: [0.1, 0.05, ...], 0.001: [0.09, 0.045, ...], ...}\n",
    "        acc_train_list_Log (list): Training accuracies for each learning rate without input normalization.\n",
    "        acc_val_list_Log (list): Validation accuracies for each learning rate without input normalization.\n",
    "        best_lr (float): Best learning rate based on validation accuracy without input normalization.\n",
    "        acc_test (float): Test accuracy for the best learning rate without input normalization.\n",
    "        cost_dict_norm (dict): Dictionary of cost values for each learning rate with input normalization.\n",
    "        acc_train_list_Log_norm (list): Training accuracies for each learning rate with input normalization.\n",
    "        acc_val_list_Log_norm (list): Validation accuracies for each learning rate with input normalization.\n",
    "        best_lr_norm (float): Best learning rate based on validation accuracy with input normalization.\n",
    "        acc_test_norm (float): Test accuracy for the best learning rate with input normalization.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # your code goes here\n",
    "\n",
    "\n",
    "    # return in this order      \n",
    "\n",
    "    return cost_dict,acc_train_list_Log, acc_val_list_Log, best_lr,test_acc_Log, cost_dict_norm, acc_train_list_Log_norm, acc_val_list_Log_norm, best_lr_norm, test_acc_Log_norm\n",
    "\n",
    "def cost_vs_iter_curve (cost_dict, cost_dict_norm):\n",
    "     \"\"\"\n",
    "    Args:\n",
    "        cost_dict (dict): Dictionary of cost values for each learning rate without input normalization.\n",
    "        cost_dict_norm (dict): Dictionary of cost values for each learning rate with input normalization.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # your code goes here\n",
    "\n",
    "\n",
    "\n",
    "cost_dict,acc_train_list_Log, acc_val_list_Log, best_lr,test_acc_Log, cost_dict_norm, acc_train_list_Log_norm, acc_val_list_Log_norm, best_lr_norm, test_acc_Log_norm = MLR_select_lr(FS_X_train, FS_X_val, FS_X_test, Ytr_onehot, Yval_onehot, Yts_onehot)\n",
    "\n",
    "print(f\"Without Normalization\")\n",
    "print(f\"Training accuracies for different learning rates: {np.round(acc_train_list_Log,2)}\")\n",
    "print(f\"Validation accuracies for different learning rates: {np.round(acc_val_list_Log,2)}\")\n",
    "print(f\"Best learning rate: {best_lr}\")\n",
    "print(f\"Test accuracy for best learning rate {best_lr}: {np.round(test_acc_Log,2)}\")\n",
    "\n",
    "\n",
    "print(f\"With Z-score Standardization\")\n",
    "print(f\"Training accuracies for different learning rates: {np.round(acc_train_list_Log_norm,2)}\")\n",
    "print(f\"Validation accuracies for different learning rates: {np.round(acc_val_list_Log_norm,2)}\")\n",
    "print(f\"Best learning rate: {best_lr_norm}\")\n",
    "print(f\"Test accuracy for best learning rate {best_lr_norm}: {np.round(test_acc_Log_norm,2)}\")\n",
    "\n",
    "cost_vs_iter_curve (cost_dict, cost_dict_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8502b5a1",
   "metadata": {},
   "source": [
    "### Analysis of Effect of Normalization Based on Your Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e054b4",
   "metadata": {},
   "source": [
    "#### Add Markdown cell below to analyze the effect of normalization based on your results\n",
    "<span style=\"color:orange\">(delete this markdown cell before submission)</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ee2211-2213",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
