{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93a15b08",
   "metadata": {},
   "source": [
    "# EE2213 Project: Multi-Class Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc894ae",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">No additional library imports are permitted.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7df0339",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sympy as sp\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler, OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539146d0",
   "metadata": {},
   "source": [
    "## PART 0: Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de654f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape: (846, 18), target output shape:(846,)\n",
      "feature names: ['COMPACTNESS', 'CIRCULARITY', 'DISTANCE_CIRCULARITY', 'RADIUS_RATIO', 'PR.AXIS_ASPECT_RATIO', 'MAX.LENGTH_ASPECT_RATIO', 'SCATTER_RATIO', 'ELONGATEDNESS', 'PR.AXIS_RECTANGULARITY', 'MAX.LENGTH_RECTANGULARITY', 'SCALED_VARIANCE_MAJOR', 'SCALED_VARIANCE_MINOR', 'SCALED_RADIUS_OF_GYRATION', 'SKEWNESS_ABOUT_MAJOR', 'SKEWNESS_ABOUT_MINOR', 'KURTOSIS_ABOUT_MAJOR', 'KURTOSIS_ABOUT_MINOR', 'HOLLOWS_RATIO']\n"
     ]
    }
   ],
   "source": [
    "def load_openml_dataset():\n",
    "\n",
    "    dataset = fetch_openml(name='vehicle', version=1, as_frame=True, parser='auto')\n",
    "    X = dataset.data.values\n",
    "    \n",
    "    target_values = dataset.target.values\n",
    "    unique_targets = np.unique(target_values)\n",
    "    \n",
    "    # Create mapping from string labels to integers\n",
    "    label_to_int = {label: i for i, label in enumerate(unique_targets)}\n",
    "    y = np.array([label_to_int[label] for label in target_values])\n",
    "    \n",
    "    feature_names = dataset.feature_names\n",
    "    \n",
    "    return X, y, feature_names\n",
    "\n",
    "X, y, feature_names = load_openml_dataset()\n",
    "print(f\"Feature shape: {X.shape}, target output shape:{y.shape}\")\n",
    "print(f\"feature names: {feature_names}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ab9a4e",
   "metadata": {},
   "source": [
    "## PART 1: Dataset Partition and One-hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ec66350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (507, 18), (507, 4)\n",
      "Validation set shape: (169, 18), (169, 4)\n",
      "Test set shape: (170, 18), (170, 4)\n"
     ]
    }
   ],
   "source": [
    "def dataset_partition_encoding(X, y):\n",
    "    \"\"\"\n",
    "    Input type\n",
    "    :X type: numpy.ndarray of size (number_of_samples, number_of_features)\n",
    "    :y type: numpy.ndarray of size (number_of_samples,)\n",
    "\n",
    "    Return type\n",
    "    :X_train type: numpy.ndarray of size (number_of_training_samples, number_of_features)\n",
    "    :X_val type: numpy.ndarray of size (number_of_validation_samples, number_of_features)\n",
    "    :X_test type: numpy.ndarray of size (number_of_test_samples, number_of_features)\n",
    "    :Ytr_onehot type: numpy.ndarray of size (number_of_training_samples, num_classes)\n",
    "    :Yval_onehot type: numpy.ndarray of size (number_of_validation_samples, num_classes)\n",
    "    :Yts_onehot type: numpy.ndarray of size (number_of_test_samples, num_classes)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # your code goes here\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=665, stratify=y)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=665, stratify=y_temp)\n",
    "    \n",
    "    encoder = OneHotEncoder(sparse_output=False)\n",
    "    Ytr_onehot = encoder.fit_transform(y_train.reshape(-1, 1))\n",
    "    Yval_onehot = encoder.transform(y_val.reshape(-1, 1))\n",
    "    Yts_onehot = encoder.transform(y_test.reshape(-1, 1))\n",
    "    \n",
    "\n",
    "    # return in this order\n",
    "    return X_train, X_val, X_test, Ytr_onehot, Yval_onehot, Yts_onehot\n",
    "\n",
    "X_train, X_val, X_test, Ytr_onehot, Yval_onehot, Yts_onehot = dataset_partition_encoding(X, y)\n",
    "print(f\"Training set shape: {X_train.shape}, {Ytr_onehot.shape}\")\n",
    "print(f\"Validation set shape: {X_val.shape}, {Yval_onehot.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}, {Yts_onehot.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f9bba7",
   "metadata": {},
   "source": [
    "## PART 2: Feature Selection using Pearson Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f793e2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 Selected Features: ['COMPACTNESS', 'CIRCULARITY', 'DISTANCE_CIRCULARITY', 'RADIUS_RATIO', 'PR.AXIS_ASPECT_RATIO', 'MAX.LENGTH_ASPECT_RATIO', 'SKEWNESS_ABOUT_MAJOR', 'SKEWNESS_ABOUT_MINOR', 'KURTOSIS_ABOUT_MAJOR', 'KURTOSIS_ABOUT_MINOR']\n",
      "Training set shape after feature selection: (507, 10), (507, 4)\n",
      "Validation set shape after feature selection: (169, 10), (169, 4)\n",
      "Test set shape after feature selection: (170, 10), (170, 4)\n"
     ]
    }
   ],
   "source": [
    "def feature_selection(X_train, X_val, X_test, feature_names, threshold=0.8):\n",
    "    \"\"\"\n",
    "    Input type\n",
    "    :X_train type: numpy.ndarray of size (number_of_training_samples, number_of_features)\n",
    "    :X_val type: numpy.ndarray of size (number_of_validation_samples, number_of_features)\n",
    "    :X_test type: numpy.ndarray of size (number_of_test_samples, number_of_features)\n",
    "    :feature_names type: list of str\n",
    "    :threshold type: float\n",
    "\n",
    "    Return type\n",
    "    :selected_features type: list of str\n",
    "    :FS_X_train type: numpy.ndarray of size (number_of_training_samples, number_of_selected_features)\n",
    "    :FS_X_val type: numpy.ndarray of size (number_of_validation_samples, number_of_selected_features)\n",
    "    :FS_X_test type: numpy.ndarray of size (number_of_test_samples, number_of_selected_features)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # your code goes here\n",
    "    corr_matrix = np.corrcoef(X_train, rowvar=False)\n",
    "\n",
    "    to_keep = [0]\n",
    "\n",
    "    for i in range(1, X_train.shape[1]):\n",
    "        is_correlated = False\n",
    "        for j in to_keep:\n",
    "            if abs(corr_matrix[i, j]) > threshold:\n",
    "                is_correlated = True\n",
    "                break\n",
    "        if not is_correlated:\n",
    "            to_keep.append(i)\n",
    "\n",
    "    selected_features = [feature_names[i] for i in to_keep]\n",
    "    FS_X_train = X_train[:, to_keep]\n",
    "    FS_X_val = X_val[:, to_keep]\n",
    "    FS_X_test = X_test[:, to_keep]\n",
    "\n",
    "    # return in this order\n",
    "    return selected_features, FS_X_train, FS_X_val, FS_X_test\n",
    "\n",
    "\n",
    "selected_features, FS_X_train, FS_X_val, FS_X_test = feature_selection(X_train, X_val, X_test, feature_names)\n",
    "\n",
    "\n",
    "print(f\"{len(selected_features)} Selected Features: {selected_features}\")\n",
    "print(f\"Training set shape after feature selection: {FS_X_train.shape}, {Ytr_onehot.shape}\")\n",
    "print(f\"Validation set shape after feature selection: {FS_X_val.shape}, {Yval_onehot.shape}\")\n",
    "print(f\"Test set shape after feature selection: {FS_X_test.shape}, {Yts_onehot.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c76d902",
   "metadata": {},
   "source": [
    "## PART 3: Polynomial Feature Transformation and Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b0de36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracies: [0.72 0.88 0.96]\n",
      "Validation accuracies: [0.71 0.74 0.72]\n",
      "Best polynomial order: 2\n",
      "Test accuracy for best order 2: 0.91\n"
     ]
    }
   ],
   "source": [
    "def polynomial_for_classification(FS_X_train, FS_X_val, FS_X_test, Ytr_onehot, Yval_onehot, Yts_onehot, max_order=3, lamda=0.001):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        FS_X_train (np.ndarray): Feature matrix for training.\n",
    "        FS_X_val (np.ndarray): Feature matrix for validation.\n",
    "        FS_X_test (np.ndarray): Feature matrix for testing.\n",
    "        Ytr_onehot (np.ndarray): One-hot encoded labels for training.\n",
    "        Yval_onehot (np.ndarray): One-hot encoded labels for validation.\n",
    "        Yts_onehot (np.ndarray): One-hot encoded labels for testing.\n",
    "        max_order (int): Maximum polynomial order to consider.\n",
    "        lamda (float): Regularization strength.\n",
    "\n",
    "    Returns:\n",
    "        acc_train_list (list): Training accuracies for each polynomial order.\n",
    "        acc_val_list (list): Validation accuracies for each polynomial order.\n",
    "        best_order (int): Best polynomial order based on validation accuracy.\n",
    "        acc_test (float): Test accuracy for the best polynomial order.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # your code goes here\n",
    "    acc_train_list = []\n",
    "    acc_val_list = []\n",
    "    acc_test = 0.0\n",
    "    best_order = None\n",
    "    best_W = None\n",
    "\n",
    "    for order in range(1, 4):\n",
    "        features = PolynomialFeatures(degree=order)\n",
    "        poly_tr = features.fit_transform(FS_X_train)\n",
    "        poly_val = features.transform(FS_X_val)\n",
    "        \n",
    "        r, c = poly_tr.shape\n",
    "\n",
    "        if r > c:\n",
    "            XtX = poly_tr.T @ poly_tr\n",
    "            try:\n",
    "                W = np.linalg.inv(XtX) @ poly_tr.T @ Ytr_onehot\n",
    "            except np.linalg.LinAlgError:\n",
    "                W = np.linalg.inv(XtX + lamda * np.eye(c)) @ poly_tr.T @ Ytr_onehot\n",
    "\n",
    "        elif r < c:\n",
    "            XXt = poly_tr @ poly_tr.T\n",
    "            try:\n",
    "                W = poly_tr.T @ np.linalg.inv(XXt) @ Ytr_onehot\n",
    "            except np.linalg.LinAlgError:\n",
    "                W = poly_tr.T @ np.linalg.inv(XXt + lamda * np.eye(r)) @ Ytr_onehot\n",
    "\n",
    "        Ytr_pred = np.argmax(poly_tr @ W, axis=1)\n",
    "        Yval_pred = np.argmax(poly_val @ W, axis=1)\n",
    "\n",
    "        Ytr_true = np.argmax(Ytr_onehot, axis=1)\n",
    "        Yval_true = np.argmax(Yval_onehot, axis=1)\n",
    "\n",
    "        acc_train_list.append(accuracy_score(Ytr_true, Ytr_pred))\n",
    "        acc_val_list.append(accuracy_score(Yval_true, Yval_pred))\n",
    "\n",
    "        if best_order is None or acc_val_list[-1] > acc_val_list[-2]:\n",
    "            best_order = order\n",
    "            best_W = W\n",
    "\n",
    "    best = PolynomialFeatures(degree=best_order)\n",
    "    poly_test = best.fit_transform(FS_X_test)\n",
    "    Yts_pred = np.argmax(poly_test @ best_W, axis=1)\n",
    "    Yts_true = np.argmax(Yts_onehot, axis=1)\n",
    "\n",
    "    acc_test = accuracy_score(Yts_true, Yts_pred)\n",
    "\n",
    "    # return in this order              \n",
    "    return acc_train_list, acc_val_list, best_order, acc_test\n",
    "\n",
    "acc_train_list, acc_val_list, best_order, acc_test = polynomial_for_classification(FS_X_train, FS_X_val, FS_X_test, Ytr_onehot, Yval_onehot, Yts_onehot)\n",
    "\n",
    "print(f\"Training accuracies: {np.round(acc_train_list,2)}\")\n",
    "print(f\"Validation accuracies: {np.round(acc_val_list,2)}\")\n",
    "print(f\"Best polynomial order: {best_order}\")\n",
    "print(f\"Test accuracy for best order {best_order}: {np.round(acc_test,2)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c8775e",
   "metadata": {},
   "source": [
    "## PART 4: Multinomial Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2dd90cef",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cost_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 47\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;250m     \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[33;03m        cost_dict (dict): Dictionary of cost values for each learning rate without input normalization.\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[33;03m        cost_dict_norm (dict): Dictionary of cost values for each learning rate with input normalization.\u001b[39;00m\n\u001b[32m     40\u001b[39m \n\u001b[32m     41\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m     43\u001b[39m     \u001b[38;5;66;03m# your code goes here\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m cost_dict,acc_train_list_Log, acc_val_list_Log, best_lr,test_acc_Log, cost_dict_norm, acc_train_list_Log_norm, acc_val_list_Log_norm, best_lr_norm, test_acc_Log_norm = \u001b[43mMLR_select_lr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mFS_X_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mFS_X_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mFS_X_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mYtr_onehot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mYval_onehot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mYts_onehot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mWithout Normalization\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     50\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTraining accuracies for different learning rates: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp.round(acc_train_list_Log,\u001b[32m2\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36mMLR_select_lr\u001b[39m\u001b[34m(FS_X_train, FS_X_val, FS_X_test, Ytr_onehot, Yval_onehot, Yts_onehot, lr_list, num_iters)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03m    FS_X_train (np.ndarray): Feature matrix for training.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     25\u001b[39m \n\u001b[32m     26\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# your code goes here\u001b[39;00m\n\u001b[32m     29\u001b[39m \n\u001b[32m     30\u001b[39m \n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# return in this order      \u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcost_dict\u001b[49m,acc_train_list_Log, acc_val_list_Log, best_lr,test_acc_Log, cost_dict_norm, acc_train_list_Log_norm, acc_val_list_Log_norm, best_lr_norm, test_acc_Log_norm\n",
      "\u001b[31mNameError\u001b[39m: name 'cost_dict' is not defined"
     ]
    }
   ],
   "source": [
    "def MLR_select_lr(FS_X_train, FS_X_val, FS_X_test, Ytr_onehot, Yval_onehot, Yts_onehot, lr_list=[0.0001, 0.001, 0.01, 0.1], num_iters=20000):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        FS_X_train (np.ndarray): Feature matrix for training.\n",
    "        FS_X_val (np.ndarray): Feature matrix for validation.\n",
    "        FS_X_test (np.ndarray): Feature matrix for testing.\n",
    "        Ytr_onehot (np.ndarray): One-hot encoded labels for training.\n",
    "        Yval_onehot (np.ndarray): One-hot encoded labels for validation.\n",
    "        Yts_onehot (np.ndarray): One-hot encoded labels for testing.\n",
    "        lr_list (list): List of learning rates to test.\n",
    "        num_iters (int): Number of iterations for training.\n",
    "\n",
    "    Returns:\n",
    "        cost_dict (dict): Dictionary of cost values for each learning rate without input normalization.\n",
    "                          example: cost_dict = {0.0001: [0.1, 0.05, ...], 0.001: [0.09, 0.045, ...], ...}\n",
    "        acc_train_list_Log (list): Training accuracies for each learning rate without input normalization.\n",
    "        acc_val_list_Log (list): Validation accuracies for each learning rate without input normalization.\n",
    "        best_lr (float): Best learning rate based on validation accuracy without input normalization.\n",
    "        acc_test (float): Test accuracy for the best learning rate without input normalization.\n",
    "        cost_dict_norm (dict): Dictionary of cost values for each learning rate with input normalization.\n",
    "        acc_train_list_Log_norm (list): Training accuracies for each learning rate with input normalization.\n",
    "        acc_val_list_Log_norm (list): Validation accuracies for each learning rate with input normalization.\n",
    "        best_lr_norm (float): Best learning rate based on validation accuracy with input normalization.\n",
    "        acc_test_norm (float): Test accuracy for the best learning rate with input normalization.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # your code goes here\n",
    "\n",
    "\n",
    "    # return in this order      \n",
    "\n",
    "    return cost_dict,acc_train_list_Log, acc_val_list_Log, best_lr,test_acc_Log, cost_dict_norm, acc_train_list_Log_norm, acc_val_list_Log_norm, best_lr_norm, test_acc_Log_norm\n",
    "\n",
    "def cost_vs_iter_curve (cost_dict, cost_dict_norm):\n",
    "     \"\"\"\n",
    "    Args:\n",
    "        cost_dict (dict): Dictionary of cost values for each learning rate without input normalization.\n",
    "        cost_dict_norm (dict): Dictionary of cost values for each learning rate with input normalization.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # your code goes here\n",
    "\n",
    "\n",
    "\n",
    "cost_dict,acc_train_list_Log, acc_val_list_Log, best_lr,test_acc_Log, cost_dict_norm, acc_train_list_Log_norm, acc_val_list_Log_norm, best_lr_norm, test_acc_Log_norm = MLR_select_lr(FS_X_train, FS_X_val, FS_X_test, Ytr_onehot, Yval_onehot, Yts_onehot)\n",
    "\n",
    "print(f\"Without Normalization\")\n",
    "print(f\"Training accuracies for different learning rates: {np.round(acc_train_list_Log,2)}\")\n",
    "print(f\"Validation accuracies for different learning rates: {np.round(acc_val_list_Log,2)}\")\n",
    "print(f\"Best learning rate: {best_lr}\")\n",
    "print(f\"Test accuracy for best learning rate {best_lr}: {np.round(test_acc_Log,2)}\")\n",
    "\n",
    "\n",
    "print(f\"With Z-score Standardization\")\n",
    "print(f\"Training accuracies for different learning rates: {np.round(acc_train_list_Log_norm,2)}\")\n",
    "print(f\"Validation accuracies for different learning rates: {np.round(acc_val_list_Log_norm,2)}\")\n",
    "print(f\"Best learning rate: {best_lr_norm}\")\n",
    "print(f\"Test accuracy for best learning rate {best_lr_norm}: {np.round(test_acc_Log_norm,2)}\")\n",
    "\n",
    "cost_vs_iter_curve (cost_dict, cost_dict_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8502b5a1",
   "metadata": {},
   "source": [
    "### Analysis of Effect of Normalization Based on Your Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e054b4",
   "metadata": {},
   "source": [
    "#### Add Markdown cell below to analyze the effect of normalization based on your results\n",
    "<span style=\"color:orange\">(delete this markdown cell before submission)</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ee2211-2213",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
