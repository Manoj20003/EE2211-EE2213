{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7c7809",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits() # Load dataset as a dictionary-like object.\n",
    "print(\"The digits dataset keys:\", digits.keys())\n",
    "print(\"The digits dataset description:\\n\", digits.DESCR)\n",
    "\n",
    "X, y = digits.data, digits.target\n",
    "print(\"The digits dataset features (X) shape:\", X.shape)\n",
    "print(\"The digits dataset labels (y) shape:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe72dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot first image\n",
    "print(\"flattened vector:\", X[0])\n",
    "print(\"original image:\\n\", digits.images[0])\n",
    "print(\"label:\", y[0])\n",
    "plt.imshow(digits.images[0], cmap='gray')\n",
    "plt.title(f'Label: {digits.target[0]}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817f9808",
   "metadata": {},
   "source": [
    "## (i) ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3c89ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training, validation, and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler() # Creates a scaler object that standardizes features to have mean = 0 and std = 1.\n",
    "X_train = scaler.fit_transform(X_train) # Fit and transform the training data\n",
    "X_val = scaler.transform(X_val) # Transform the validation data\n",
    "X_test = scaler.transform(X_test) # Transform the test data\n",
    "\n",
    "# Print the shapes of the datasets\n",
    "print(f\"Training set shape: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Validation set shape: {X_val.shape}, {y_val.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}, {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a2ceba",
   "metadata": {},
   "source": [
    "## (ii) ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dabad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse_output=False)\n",
    "reshaped = y_train.reshape(len(y_train), 1) # 1D array to 2D array\n",
    "Ytr_onehot = onehot_encoder.fit_transform(reshaped)\n",
    "\n",
    "reshaped = y_val.reshape(len(y_val), 1)\n",
    "Yval_onehot = onehot_encoder.fit_transform(reshaped)\n",
    "\n",
    "reshaped = y_test.reshape(len(y_test), 1)\n",
    "Yts_onehot = onehot_encoder.fit_transform(reshaped)\n",
    "\n",
    "Yts_onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81d722d",
   "metadata": {},
   "source": [
    "## (iii) ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d90aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def Softmax(z,eps=1e-15):\n",
    "    z_max = np.max(z, axis=-1, keepdims=True)  # for numerical stability\n",
    "    exp_z = np.exp(z - z_max)\n",
    "    pred_Y = exp_z / np.sum(exp_z, axis=-1, keepdims=True)\n",
    "\n",
    "    # Clip predictions to prevent log(0)\n",
    "    pred_Y = np.clip(pred_Y, eps, 1 - eps)\n",
    "    \n",
    "    return pred_Y\n",
    "  \n",
    "\n",
    "def cross_entropy_cost(Y, Y_hat):\n",
    "    return -np.sum(Y * np.log(Y_hat)) / Y.shape[0]\n",
    "\n",
    "def forward_pass(X, W1, W2, W3):\n",
    "    # first layer output\n",
    "    O1=ReLU(X @ W1)\n",
    "\n",
    "    # second layer output\n",
    "    # Column of 1s for bias term\n",
    "    ones = np.ones((O1.shape[0], 1))\n",
    "    # Concatenate along columns (axis=1)\n",
    "    A2 = np.hstack((ones, O1))\n",
    "    O2 = ReLU(A2 @ W2)\n",
    "\n",
    "    # third layer output\n",
    "    ones = np.ones((O2.shape[0], 1))\n",
    "    A3 = np.hstack((ones, O2))\n",
    "    Y_hat = Softmax(A3 @ W3)\n",
    "\n",
    "    return Y_hat, A3, A2, O2, O1\n",
    "\n",
    "# backward pass for output layer\n",
    "def backward_pass_output(Y, Y_hat, A3, W3, lr):\n",
    "\n",
    "    N=Y.shape[0]\n",
    "\n",
    "    # Error at output layer\n",
    "    E3 = Y_hat-Y\n",
    "    # Gradient at output layer\n",
    "    G3 = (A3.T @ E3)/N\n",
    "    # Weights update at output layer\n",
    "    W3_new = W3 - lr * G3\n",
    "\n",
    "    return E3, G3, W3_new\n",
    "\n",
    "# backward pass for hidden layer\n",
    "def backward_pass_hidden(E_ladd1, W_ladd1, A_l, O_l, W_l, lr):\n",
    "    \n",
    "    N = A_l.shape[0]\n",
    "    # Error at hidden layer l\n",
    "    E_l = E_ladd1 @ W_ladd1[1:].T * (O_l > 0)  # ReLU derivative\n",
    "    # Gradient at hidden layer l\n",
    "    G_l = (A_l.T @ E_l)/N\n",
    "    # Weights update at hidden layer l\n",
    "    W_l_new = W_l - lr * G_l\n",
    "\n",
    "    return E_l, G_l, W_l_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc686be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP_3layer(X, Y, W1_init, W2_init, W3_init, lr, num_iters):\n",
    "    W1, W2, W3 = W1_init.copy(), W2_init.copy(), W3_init.copy() \n",
    "    # Create a copy of the initial weights to work out.\n",
    "    cost_vec = np.zeros(num_iters+1)\n",
    "    for i in range(1, num_iters+1):\n",
    "        Y_hat, A3, A2, O2, O1 = forward_pass(X, W1, W2, W3)\n",
    "        cost = cross_entropy_cost(Y, Y_hat)\n",
    "        if i % 1000 == 1:\n",
    "            print(f\"Iteration {i-1}, Cost: {cost}\")\n",
    "        cost_vec[i-1]=cost\n",
    "        E3, G3, W3_new = backward_pass_output(Y, Y_hat, A3, W3, lr) \n",
    "        E2, G2, W2_new = backward_pass_hidden(E3, W3, A2, O2, W2, lr)\n",
    "        E1, G1, W1_new = backward_pass_hidden(E2, W2, X, O1, W1, lr)\n",
    "\n",
    "        W1, W2, W3 = W1_new.copy(), W2_new.copy(), W3_new.copy()\n",
    "        # W=W_new.copy(): avoid modifying the original weights. \n",
    "        #                 Creates a new array W in memory with the same values as W_new.\n",
    "        #                 This is a deep copy, so changes to W wonâ€™t affect W_new, and vice versa.\n",
    "        # W=W_new: Only creates a new reference pointing to the same array.\n",
    "        #          Changes to W would affect W_new, because they share the same memory, and vice versa.\n",
    "\n",
    "    Y_hat, A3, A2, O2, O1 = forward_pass(X, W1, W2, W3)\n",
    "    cost = cross_entropy_cost(Y, Y_hat)\n",
    "    cost_vec[i]=cost\n",
    "    print(f\"Final cost: {cost}\")\n",
    "    \n",
    "    return Y_hat, cost_vec, W1, W2, W3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c71a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "order = 1\n",
    "# Create polynomial features X to P\n",
    "Poly = PolynomialFeatures(order)\n",
    "X_train_poly = Poly.fit_transform(X_train)\n",
    "X_val_poly = Poly.fit_transform(X_val)\n",
    "X_test_poly = Poly.fit_transform(X_test)\n",
    "\n",
    "hid_layer_size = 32\n",
    "\n",
    "# output layer size = number of classes\n",
    "output_layer_size = Ytr_onehot.shape[1]\n",
    "\n",
    "# Initialize weights\n",
    "np.random.seed(42)  # For reproducibility\n",
    "W1 = np.random.randn(X_train_poly.shape[1], hid_layer_size) # Generates random numbers from a standard normal distribution\n",
    "W2 = np.random.randn(hid_layer_size+1, hid_layer_size)\n",
    "W3 = np.random.randn(hid_layer_size+1, output_layer_size)\n",
    "\n",
    "lr=0.01\n",
    "num_iters = 20000\n",
    "\n",
    "Ytr_est, cost_vec_32, W1_new_32, W2_new_32, W3_new_32 = MLP_3layer(X_train_poly, Ytr_onehot, W1, W2, W3, lr, num_iters)\n",
    "train_acc = accuracy_score(y_train, np.argmax(Ytr_est, axis=1))\n",
    "print(f\"Training accuracy for learning rate {lr}: {train_acc}\")\n",
    "Yval_est,_,_,_,_= forward_pass(X_val_poly, W1_new_32, W2_new_32, W3_new_32)\n",
    "val_acc = accuracy_score(y_val, np.argmax(Yval_est, axis=1))\n",
    "print(f\"Validation accuracy for learning rate {lr}: {val_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d93684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cost function values over iterations for each learning rate\n",
    "plt.figure(0, figsize=[9,4.5])\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "plt.plot(np.arange(0, num_iters+1, 1), cost_vec_32, label=f'hid_size={hid_layer_size}')\n",
    "plt.legend(loc='upper right', fontsize=15)\n",
    "plt.xlabel('Iteration Number')\n",
    "plt.ylabel('Categorical Cross Entropy Cost')\n",
    "plt.xticks(np.arange(0, num_iters+1, 2000))\n",
    "plt.title('3-layer MLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400cec99",
   "metadata": {},
   "source": [
    "## (iv) ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442cc32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "order = 1\n",
    "# Create polynomial features X to P\n",
    "Poly = PolynomialFeatures(order)\n",
    "X_train_poly = Poly.fit_transform(X_train)\n",
    "X_val_poly = Poly.fit_transform(X_val)\n",
    "X_test_poly = Poly.fit_transform(X_test)\n",
    "\n",
    "hid_layer_size = 36\n",
    "output_layer_size = Ytr_onehot.shape[1]\n",
    "\n",
    "# Initialize weights\n",
    "np.random.seed(42)  # For reproducibility\n",
    "W1 = np.random.randn(X_train_poly.shape[1], hid_layer_size)\n",
    "W2 = np.random.randn(hid_layer_size+1, hid_layer_size)\n",
    "W3 = np.random.randn(hid_layer_size+1, output_layer_size)\n",
    "\n",
    "lr=0.01\n",
    "num_iters = 20000\n",
    "\n",
    "Ytr_est, cost_vec_36, W1_new_36, W2_new_36, W3_new_36 = MLP_3layer(X_train_poly, Ytr_onehot, W1, W2, W3, lr, num_iters)\n",
    "train_acc = accuracy_score(y_train, np.argmax(Ytr_est, axis=1))\n",
    "print(f\"Training accuracy for hidden layer size {hid_layer_size}: {train_acc}\")\n",
    "Yval_est,_,_,_,_= forward_pass(X_val_poly, W1_new_36, W2_new_36, W3_new_36)\n",
    "val_acc = accuracy_score(y_val, np.argmax(Yval_est, axis=1))\n",
    "print(f\"Validation accuracy for hidden layer size {hid_layer_size}: {val_acc}\")\n",
    "\n",
    "# Plot cost function values over iterations for each learning rate\n",
    "plt.figure(0, figsize=[9,4.5])\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "plt.plot(np.arange(0, num_iters+1, 1), cost_vec_32, c='b', label='hid_size=32')\n",
    "plt.plot(np.arange(0, num_iters+1, 1), cost_vec_36, c='r', label='hid_size=36')\n",
    "plt.legend(loc='upper right', fontsize=15)\n",
    "plt.xlabel('Iteration Number')\n",
    "plt.ylabel('Categorical Cross Entropy Cost')\n",
    "plt.xticks(np.arange(0, num_iters+1, 2000))\n",
    "plt.title('3-layer MLP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5946748d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on test set\n",
    "Yts_est,_,_,_,_= forward_pass(X_test_poly, W1_new_32, W2_new_32, W3_new_32)\n",
    "test_acc = accuracy_score(y_test, np.argmax(Yts_est, axis=1))\n",
    "print(f\"test accuracy for hidden layer size=32: {test_acc}\")\n",
    "cm_test = confusion_matrix(y_test, np.argmax(Yts_est, axis=1))\n",
    "print(\"Confusion Matrix for Test Set:\")\n",
    "print(cm_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7884112a",
   "metadata": {},
   "source": [
    "### Use sklearn's library ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a73eb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "hid_layer_size_list = [32, 36]\n",
    "train_acc_list = {}\n",
    "val_acc_list = {}\n",
    "max_val_acc = 0\n",
    "best_size = 0\n",
    "\n",
    "for hid_layer_size in hid_layer_size_list:\n",
    "    # Define MLP and fit once to initialize shapes\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(hid_layer_size, hid_layer_size), \n",
    "                        solver='sgd',\n",
    "                        max_iter=num_iters, \n",
    "                        learning_rate_init=lr,\n",
    "                        random_state=42)\n",
    "    mlp.fit(X_train, y_train)\n",
    "\n",
    "    ytr_pred = mlp.predict(X_train)\n",
    "    train_acc = accuracy_score(y_train, ytr_pred)\n",
    "    print(f\"Training accuracy for hidden layer size {hid_layer_size}: {train_acc}\")\n",
    "    train_acc_list[hid_layer_size]=train_acc\n",
    "\n",
    "    yval_pred = mlp.predict(X_val)\n",
    "    val_acc = accuracy_score(y_val, yval_pred)\n",
    "    print(f\"Validation accuracy for hidden layer size {hid_layer_size}: {val_acc}\")\n",
    "    val_acc_list[hid_layer_size]=val_acc\n",
    "\n",
    "    if val_acc > max_val_acc:\n",
    "        max_val_acc = val_acc\n",
    "        best_size = hid_layer_size\n",
    "        yts_pred=mlp.predict(X_test)\n",
    "        test_acc = accuracy_score(y_test, yts_pred)\n",
    "\n",
    "print(f\"Best hidden layer size: {best_size}, Max validation accuracy: {max_val_acc}\")\n",
    "print(f\"Test accuracy: {test_acc}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ee2213",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
