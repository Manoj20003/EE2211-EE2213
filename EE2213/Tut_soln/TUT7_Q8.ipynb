{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe7c7809",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([[1,2,1],[1,5,1]]) # 2 samples, 3 input features\n",
    "y = np.array([[0.1],[0.7]])\n",
    "\n",
    "# original weights\n",
    "W1=np.array([[-1,0],[0,1],[1,-1]]) # 2 neurons for the 1st layer\n",
    "W2=np.array([[1],[-1],[1]]) # 1 neuron for the output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4d90aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def ReLU_derivative(o):\n",
    "    return (o > 0).astype(float)\n",
    "  \n",
    "def squared_error_loss_derivative(y, y_hat):\n",
    "    return 2*(y_hat-y)\n",
    "\n",
    "def forward_pass(X, W1, W2):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    X: Input data, dimensions of n_samples x n_features\n",
    "    W1: Weights for the first layer, dimensions of n_features x n_neurons1\n",
    "    W2: Weights for the second layer, dimensions of n_neurons1 x n_neurons2\n",
    "\n",
    "    OUTPUT:\n",
    "    y_hat: Predicted output, dimensions of n_samples x n_neurons2\n",
    "    A2: Inputs to the second layer, dimensions of n_samples x (n_neurons1+1)\n",
    "    O1: Outputs at the first layer, dimensions of n_samples x n_neurons1\n",
    "    \"\"\"\n",
    "    # first layer output\n",
    "    O1=ReLU(X @ W1)\n",
    "\n",
    "    # second layer output\n",
    "    # Column of 1s for bias term\n",
    "    ones = np.ones((O1.shape[0], 1))\n",
    "    # Concatenate along columns (axis=1)\n",
    "    A2 = np.hstack((ones, O1))\n",
    "    y_hat = A2 @ W2\n",
    "\n",
    "    return y_hat, A2, O1\n",
    "\n",
    "def backward_pass_output(y, y_hat, A_o, W_o, lr):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    y: ground truth, dimensions of n_samples x n_neurons\n",
    "    y_hat: outputs of the output layer\n",
    "    A_o: input to the output layer, dimensions of n_samples x (n_neurons+1)\n",
    "    W_o: weight at output layer\n",
    "    lr: learning rate\n",
    "\n",
    "    OUTPUT:\n",
    "    E_o: Error at output layer\n",
    "    G_o: Gradient at output layer\n",
    "    W_o_new: Updated weights at output layer\n",
    "    \"\"\"\n",
    "    N = y.shape[0]\n",
    "\n",
    "    # Error at output layer\n",
    "    E_o = squared_error_loss_derivative(y, y_hat)\n",
    "    # Gradient at output layer\n",
    "    G_o = (A_o.T @ E_o)/N\n",
    "    # Weights update at output layer\n",
    "    W_o_new = W_o - lr * G_o\n",
    "\n",
    "    return E_o, G_o, W_o_new\n",
    "\n",
    "def backward_pass_hidden(E_ladd1, W_ladd1, A_l, O_l, W_l, lr):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    E_ladd1: Error at layer l+1, dimensions of n_samples x n_neurons_(l+1)\n",
    "    W_ladd1: Weights at layer l+1, dimensions of (n_neurons_l+1) x n_neurons_(l+1)\n",
    "    A_l: inputs to layer l, dimensions of n_samples x (n_neurons_l+1)\n",
    "    O_l: Outputs at layer l, dimensions of n_samples x n_neurons_l\n",
    "    W_l: Weights at layer l, dimensions of (n_neurons_(l-1)+1) x n_neurons_l\n",
    "    lr: Learning rate\n",
    "\n",
    "    OUTPUT:\n",
    "    E_l: Error at hidden layer l, dimensions of n_samples x n_neurons_l\n",
    "    G_l: Gradient at hidden layer l, dimensions of (n_neurons_(l-1)+1) x n_neurons_l\n",
    "    W_l_new: Updated weights at hidden layer l, dimensions of (n_neurons_(l-1)+1) x n_neurons_l\n",
    "    \"\"\"\n",
    "    N = A_l.shape[0]\n",
    "    # Error at hidden layer l\n",
    "    E_l = E_ladd1 @ W_ladd1[1:].T * ReLU_derivative(O_l)  # ReLU derivative\n",
    "    # Gradient at hidden layer l\n",
    "    G_l = (A_l.T @ E_l)/N\n",
    "    # Weights update at hidden layer l\n",
    "    W_l_new = W_l - lr * G_l\n",
    "\n",
    "    return E_l, G_l, W_l_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbc686be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted output is [[2.]\n",
      " [5.]]\n",
      "Input to output layer A2 is [[1. 0. 1.]\n",
      " [1. 0. 4.]]\n",
      "Error at output layer E2 is [[3.8]\n",
      " [8.6]]\n",
      "Gradient at output layer G2 is [[ 6.2]\n",
      " [ 0. ]\n",
      " [19.1]]\n",
      "Updated W2 is [[ 0.38]\n",
      " [-1.  ]\n",
      " [-0.91]]\n",
      "Error at hidden layer E1 is [[-0.   3.8]\n",
      " [-0.   8.6]]\n",
      "Gradient at hidden layer G1 is [[ 0.   6.2]\n",
      " [ 0.  25.3]\n",
      " [ 0.   6.2]]\n",
      "Updated W1 is [[-1.   -0.62]\n",
      " [ 0.   -1.53]\n",
      " [ 1.   -1.62]]\n"
     ]
    }
   ],
   "source": [
    "lr=0.1\n",
    "\n",
    "y_hat, A2, O1=forward_pass(X, W1, W2)\n",
    "print(f'Predicted output is {y_hat}')\n",
    "print(f'Input to output layer A2 is {A2}')\n",
    "\n",
    "E2, G2, W2_new = backward_pass_output(y, y_hat, A2, W2, lr)\n",
    "print(f'Error at output layer E2 is {E2}')\n",
    "print(f'Gradient at output layer G2 is {G2}')\n",
    "print(f'Updated W2 is {W2_new}')\n",
    "\n",
    "E1, G1, W1_new = backward_pass_hidden(E2, W2, X, O1, W1, lr)\n",
    "print(f'Error at hidden layer E1 is {E1}')\n",
    "print(f'Gradient at hidden layer G1 is {G1}')\n",
    "print(f'Updated W1 is {W1_new}')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ee2213",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
