{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a484758f",
   "metadata": {},
   "source": [
    "# Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "974d345b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix rank is : 3\n",
      "matrix size is : (10, 10)\n",
      "matrix is not invertible\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import inv,matrix_rank,det\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "X = np.array([[1, 1.5],[0.9, 2], [3, 1]])\n",
    "y = np.array([1, 2.6, 3])\n",
    "order = 3\n",
    "poly = PolynomialFeatures(order)\n",
    "X_poly = poly.fit_transform(X)\n",
    "# check if X^T * X is invertible\n",
    "def check_inverse_rank(matrix, tol=1e-15):\n",
    "    rank = matrix_rank(matrix)\n",
    "    print(\"matrix rank is : \"+ str(rank))\n",
    "    print(\"matrix size is : \"+ str(matrix.shape))\n",
    "\n",
    "    if matrix.shape[0] == matrix.shape[1]:\n",
    "       if rank == matrix.shape[0]:\n",
    "           print(\"matrix is invertible\")\n",
    "       else:\n",
    "           print(\"matrix is not invertible\")\n",
    "    else:\n",
    "       print(\"matrix is not square, hence not invertible\")\n",
    "\n",
    "    return (rank == matrix.shape[0]) and (matrix.shape[0] == matrix.shape[1])\n",
    "check_inverse_rank(X_poly.T @ X_poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5c4599",
   "metadata": {},
   "source": [
    "# Q8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a46cdba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix rank is : 4\n",
      "matrix size is : (4, 4)\n",
      "matrix is invertible\n",
      "[6.66030534]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[3, 15, 50],[5, 12, 68], [2, 9, 60], [4, 19, 80]])\n",
    "y = np.array([5,6,3,8])\n",
    "lamda = 0.1\n",
    "order = 1\n",
    "poly = PolynomialFeatures(order)\n",
    "X_poly = poly.fit_transform(X)\n",
    "# check if X^T * X is invertible\n",
    "if check_inverse_rank(X_poly.T @ X_poly):\n",
    "    w = inv(X_poly.T @ X_poly) @ X_poly.T @ y\n",
    "else:\n",
    "    reg_L = lamda*np.identity(X_poly.shape[1])\n",
    "    w = inv(X_poly.T @ X_poly + reg_L) @ X_poly.T @ y\n",
    "\n",
    "# prediction\n",
    "x_test = np.array([[7, 10, 70]])\n",
    "x_test_poly = poly.fit_transform(x_test)\n",
    "y_test = x_test_poly @ w\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174494c9",
   "metadata": {},
   "source": [
    "# Q9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1867c9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_logistic_cost_gradient(X, W, Y, eps=1e-15):\n",
    "\n",
    "    # Compute prediction, cost and gradient based on cross entropy\n",
    "    z = X @ W\n",
    "    exp_z = np.exp(z)\n",
    "    pred_Y = exp_z / np.sum(exp_z, axis=-1, keepdims=True)\n",
    "    cost   = np.sum(-(Y * np.log(pred_Y)))/X.shape[0]\n",
    "    gradient = X.T @ (pred_Y-Y) / X.shape[0]\n",
    "\n",
    "    return pred_Y, cost, gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30e4ff41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated weights after 1st iteration: \n",
      " [[ 0.01625831 -0.00835474 -0.00790358]\n",
      " [ 0.00756886 -0.05825139  0.07068253]\n",
      " [ 0.09301149  0.03591212 -0.04892361]\n",
      " [-0.06720425  0.05104296  0.01616129]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# Step 0: prepare data\n",
    "X = np.array([[0.5, 1.2, -0.3],[-1, 0.8, 1.5],[2.3, -0.7, 0.5],[0, 1.5, -1]])\n",
    "poly = PolynomialFeatures(1)\n",
    "P = poly.fit_transform(X)\n",
    "\n",
    "y_raw = np.array([[1], [2], [3], [1]])\n",
    "onehot_encoder=OneHotEncoder(sparse_output=False)\n",
    "Y = onehot_encoder.fit_transform(y_raw)\n",
    "\n",
    "lr = 0.1\n",
    "# Step 1: initialize weights\n",
    "W = np.array([[0, 0, 0],[0.01, -0.02, 0.03],[0.05, 0.04, -0.01],[-0.03, 0.02, 0.01]])\n",
    "\n",
    "# Step 2: perform one step of gradient descent\n",
    "pred_Y, cost, gradient = multi_logistic_cost_gradient(P, W, Y, eps=1e-15)\n",
    "W = W - lr*gradient\n",
    "print(\"Updated weights after 1st iteration: \\n\", W)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9161cd",
   "metadata": {},
   "source": [
    "# Q10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ffc6050",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(x):\n",
    "    return np.maximum(0, x)\n",
    "def Softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=-1, keepdims=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b92532b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first layer output: [[0.074 0.032 0.   ]\n",
      " [0.    0.082 0.   ]\n",
      " [0.    0.    0.081]\n",
      " [0.105 0.04  0.   ]]\n",
      "second layer output: [[0.3336644  0.33281797 0.33351762]\n",
      " [0.33397095 0.3336972  0.33233185]\n",
      " [0.33252381 0.33387326 0.33360293]\n",
      " [0.33376095 0.3325782  0.33366084]]\n",
      "Error at Second layer: [[-0.6663356   0.33281797  0.33351762]\n",
      " [ 0.33397095 -0.6663028   0.33233185]\n",
      " [ 0.33252381  0.33387326 -0.66639707]\n",
      " [-0.66623905  0.3325782   0.33366084]]\n",
      "Gradient at Second layer: [[-0.16651997  0.08324166  0.08327831]\n",
      " [-0.02981598  0.01488731  0.01492867]\n",
      " [-0.00514667 -0.00767088  0.01281755]\n",
      " [ 0.00673361  0.00676093 -0.01349454]]\n",
      "Weights update at Second layer: [[ 0.016652   -0.00832417 -0.00832783]\n",
      " [ 0.0129816  -0.02148873  0.02850713]\n",
      " [ 0.05051467  0.04076709 -0.01128176]\n",
      " [-0.03067336  0.01932391  0.01134945]]\n",
      "Error at First layer: [[-0.00331419 -0.02333924  0.        ]\n",
      " [ 0.         -0.01327688 -0.        ]\n",
      " [-0.          0.         -0.00996222]\n",
      " [-0.00330413 -0.02334543  0.        ]]\n",
      "Gradient at First layer: [[-0.00165458 -0.01499039 -0.00249055]\n",
      " [-0.00041427  0.00040182 -0.00572828]\n",
      " [-0.0022333  -0.01841168  0.00174339]\n",
      " [ 0.0010746   0.00260797 -0.00124528]]\n",
      "Weights update at First layer: [[ 0.00016546  0.00149904  0.00024906]\n",
      " [ 0.01004143 -0.02004018  0.03057283]\n",
      " [ 0.05022333  0.04184117 -0.01017434]\n",
      " [-0.03010746  0.0197392   0.01012453]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[0.5, 1.2, -0.3],[-1, 0.8, 1.5],[2.3, -0.7, 0.5],[0, 1.5, -1]])\n",
    "poly = PolynomialFeatures(1)\n",
    "P = poly.fit_transform(X)\n",
    "\n",
    "y_raw = np.array([[1], [2], [3], [1]])\n",
    "onehot_encoder=OneHotEncoder(sparse_output=False)\n",
    "Y = onehot_encoder.fit_transform(y_raw)\n",
    "\n",
    "lr = 0.1\n",
    "# Step 1: initialize weights\n",
    "W1_0 = W2_0 = np.array([[0, 0, 0],[0.01, -0.02, 0.03],[0.05, 0.04, -0.01],[-0.03, 0.02, 0.01]])\n",
    "\n",
    "### Forward Pass ###\n",
    "# first layer output\n",
    "L1=ReLU(P @ W1_0)\n",
    "print(\"first layer output:\", L1)\n",
    "\n",
    "# second layer output\n",
    "# Column of 1s (2x1)\n",
    "ones = np.ones((L1.shape[0], 1))\n",
    "# Concatenate along columns (axis=1)\n",
    "L1_with_ones = np.hstack((ones, L1))\n",
    "L2=Softmax(L1_with_ones @ W2_0)\n",
    "print(\"second layer output:\", L2)\n",
    "\n",
    "### Backward Pass ###\n",
    "# Error at Second layer\n",
    "E2 = L2-Y\n",
    "print(\"Error at Second layer:\", E2)\n",
    "# Gradient at Second layer\n",
    "G2 = (L1_with_ones.T @ E2)/X.shape[0]\n",
    "print(\"Gradient at Second layer:\", G2)\n",
    "# Weights update at Second layer\n",
    "W2_1 = W2_0 - lr * G2\n",
    "print(\"Weights update at Second layer:\", W2_1)\n",
    "# Error at First layer\n",
    "E1 = E2 @ W2_0[1:].T * (L1 > 0) #(L1>0)=(Z1>0), which is the derivative of ReLU.\n",
    "print(\"Error at First layer:\", E1)\n",
    "# Gradient at First layer\n",
    "G1 = (P.T @ E1)/X.shape[0]\n",
    "print(\"Gradient at First layer:\", G1)\n",
    "# Weights update at First layer\n",
    "W1_1 = W1_0 - lr * G1\n",
    "print(\"Weights update at First layer:\", W1_1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ee2213",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
