{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a484758f",
   "metadata": {},
   "source": [
    "# Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "974d345b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix rank is : 4\n",
      "matrix size is : (6, 6)\n",
      "matrix is not invertible\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import inv,matrix_rank,det\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "X = np.array([[1.5, 2.0],[-0.5, 1.8], [2.1, -1.2], [0, 0.5]])\n",
    "y = np.array([4.3, 1.2, 3.7, 0.8])\n",
    "order = 2\n",
    "poly = PolynomialFeatures(order)\n",
    "X_poly = poly.fit_transform(X)\n",
    "# check if X^T * X is invertible\n",
    "def check_inverse_rank(matrix, tol=1e-15):\n",
    "    rank = matrix_rank(matrix)\n",
    "    print(\"matrix rank is : \"+ str(rank))\n",
    "    print(\"matrix size is : \"+ str(matrix.shape))\n",
    "\n",
    "    if matrix.shape[0] == matrix.shape[1]:\n",
    "       if rank == matrix.shape[0]:\n",
    "           print(\"matrix is invertible\")\n",
    "       else:\n",
    "           print(\"matrix is not invertible\")\n",
    "    else:\n",
    "       print(\"matrix is not square, hence not invertible\")\n",
    "\n",
    "    return (rank == matrix.shape[0]) and (matrix.shape[0] == matrix.shape[1])\n",
    "check_inverse_rank(X_poly.T @ X_poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5c4599",
   "metadata": {},
   "source": [
    "# Q8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a46cdba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix rank is : 4\n",
      "matrix size is : (10, 10)\n",
      "matrix is not invertible\n",
      "[8.25224645]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[6, 14, 55],[3, 18, 72], [7, 11, 65], [2, 16, 58]])\n",
    "y = np.array([7,6.5,7.5,4.5])\n",
    "lamda = 0.1\n",
    "order = 2\n",
    "poly = PolynomialFeatures(order)\n",
    "X_poly = poly.fit_transform(X)\n",
    "# check if X^T * X is invertible\n",
    "if check_inverse_rank(X_poly.T @ X_poly):\n",
    "    w = inv(X_poly.T @ X_poly) @ X_poly.T @ y\n",
    "else:\n",
    "    reg_L = lamda*np.identity(X_poly.shape[1])\n",
    "    w = inv(X_poly.T @ X_poly + reg_L) @ X_poly.T @ y\n",
    "\n",
    "# prediction\n",
    "x_test = np.array([[4, 19, 80]])\n",
    "x_test_poly = poly.fit_transform(x_test)\n",
    "y_test = x_test_poly @ w\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174494c9",
   "metadata": {},
   "source": [
    "# Q9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1867c9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_logistic_cost_gradient(X, W, Y):\n",
    "\n",
    "    # Compute prediction, cost and gradient based on cross entropy\n",
    "    z = X @ W\n",
    "    exp_z = np.exp(z)\n",
    "    pred_Y = exp_z / np.sum(exp_z, axis=-1, keepdims=True)\n",
    "    cost   = np.sum(-(Y * np.log(pred_Y)))/X.shape[0]\n",
    "    gradient = X.T @ (pred_Y-Y) / X.shape[0]\n",
    "\n",
    "    return pred_Y, cost, gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30e4ff41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated weights after 1st iteration: \n",
      " [[-0.00836671 -0.00813688  0.0165036 ]\n",
      " [-0.02076001 -0.02594286  0.08670287]\n",
      " [-0.00440691  0.00065335  0.00375356]\n",
      " [ 0.00516703  0.05342267 -0.02858971]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# Step 0: prepare data\n",
    "X = np.array([[1.2, -0.4, 0.8],[-0.6, 2.0, -0.5],[0.3, -1.2, 1.7],[2.1, 0.5, -0.8]])\n",
    "poly = PolynomialFeatures(1)\n",
    "P = poly.fit_transform(X)\n",
    "\n",
    "y_raw = np.array([[3], [1], [2], [3]])\n",
    "onehot_encoder=OneHotEncoder(sparse_output=False)\n",
    "Y = onehot_encoder.fit_transform(y_raw)\n",
    "\n",
    "lr = 0.1\n",
    "# Step 1: initialize weights\n",
    "W = np.array([[0, 0, 0],[0.02, -0.01, 0.03],[-0.05, 0.04, 0.01],[0.03, 0.02, -0.02]])\n",
    "\n",
    "# Step 2: perform one step of gradient descent\n",
    "pred_Y, cost, gradient = multi_logistic_cost_gradient(P, W, Y)\n",
    "W = W - lr*gradient\n",
    "print(\"Updated weights after 1st iteration: \\n\", W)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9161cd",
   "metadata": {},
   "source": [
    "# Q10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ffc6050",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(x):\n",
    "    return np.maximum(0, x)\n",
    "def Softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=-1, keepdims=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b92532b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first layer output: [[0.068 0.    0.016]\n",
      " [0.    0.076 0.012]\n",
      " [0.117 0.    0.   ]\n",
      " [0.    0.    0.084]]\n",
      "second layer output: [[0.33359104 0.33285795 0.33355101]\n",
      " [0.33214751 0.33438706 0.33346543]\n",
      " [0.33359278 0.33242392 0.33398331]\n",
      " [0.33389325 0.3336129  0.33249384]]\n",
      "Error at Second layer: [[ 0.33359104  0.33285795 -0.66644899]\n",
      " [-0.66785249  0.33438706  0.33346543]\n",
      " [ 0.33359278 -0.66757608  0.33398331]\n",
      " [ 0.33389325  0.3336129  -0.66750616]]\n",
      "Gradient at Second layer: [[ 0.08330615  0.08332046 -0.1666266 ]\n",
      " [ 0.01542864 -0.01386802 -0.00156062]\n",
      " [-0.0126892   0.00635335  0.00633584]\n",
      " [ 0.00634257  0.00934046 -0.01568303]]\n",
      "Weights update at Second layer: [[-0.00833061 -0.00833205  0.01666266]\n",
      " [ 0.01845714 -0.0086132   0.03015606]\n",
      " [-0.04873108  0.03936466  0.00936642]\n",
      " [ 0.02936574  0.01906595 -0.0184317 ]]\n",
      "Error at First layer: [[-0.01665023 -0.          0.02999387]\n",
      " [-0.          0.05010276 -0.02001714]\n",
      " [ 0.02336712 -0.         -0.        ]\n",
      " [-0.         -0.          0.03003918]]\n",
      "Gradient at First layer: [[ 0.00167922  0.01252569  0.01000398]\n",
      " [-0.00324253 -0.00751541  0.0277713 ]\n",
      " [-0.00534511  0.02505138 -0.00925306]\n",
      " [ 0.00660098 -0.00626285  0.00249308]]\n",
      "Weights update at First layer: [[-0.00016792 -0.00125257 -0.0010004 ]\n",
      " [ 0.02032425 -0.00924846  0.02722287]\n",
      " [-0.04946549  0.03749486  0.01092531]\n",
      " [ 0.0293399   0.02062628 -0.02024931]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[1.2, -0.4, 0.8],[-0.6, 2.0, -0.5],[0.3, -1.2, 1.7],[2.1, 0.5, -0.8]])\n",
    "poly = PolynomialFeatures(1)\n",
    "P = poly.fit_transform(X)\n",
    "\n",
    "y_raw = np.array([[3], [1], [2], [3]])\n",
    "onehot_encoder=OneHotEncoder(sparse_output=False)\n",
    "Y = onehot_encoder.fit_transform(y_raw)\n",
    "\n",
    "lr = 0.1\n",
    "# Step 1: initialize weights\n",
    "W1_0 = W2_0 = np.array([[0, 0, 0],[0.02, -0.01, 0.03],[-0.05, 0.04, 0.01],[0.03, 0.02, -0.02]])\n",
    "\n",
    "### Forward Pass ###\n",
    "# first layer output\n",
    "L1=ReLU(P @ W1_0)\n",
    "print(\"first layer output:\", L1)\n",
    "\n",
    "# second layer output\n",
    "# Column of 1s (2x1)\n",
    "ones = np.ones((L1.shape[0], 1))\n",
    "# Concatenate along columns (axis=1)\n",
    "L1_with_ones = np.hstack((ones, L1))\n",
    "L2=Softmax(L1_with_ones @ W2_0)\n",
    "print(\"second layer output:\", L2)\n",
    "\n",
    "### Backward Pass ###\n",
    "# Error at Second layer\n",
    "E2 = L2-Y\n",
    "print(\"Error at Second layer:\", E2)\n",
    "# Gradient at Second layer\n",
    "G2 = (L1_with_ones.T @ E2)/X.shape[0]\n",
    "print(\"Gradient at Second layer:\", G2)\n",
    "# Weights update at Second layer\n",
    "W2_1 = W2_0 - lr * G2\n",
    "print(\"Weights update at Second layer:\", W2_1)\n",
    "# Error at First layer\n",
    "E1 = E2 @ W2_0[1:].T * (L1 > 0) #(L1>0)=(Z1>0), which is the derivative of ReLU.\n",
    "print(\"Error at First layer:\", E1)\n",
    "# Gradient at First layer\n",
    "G1 = (P.T @ E1)/X.shape[0]\n",
    "print(\"Gradient at First layer:\", G1)\n",
    "# Weights update at First layer\n",
    "W1_1 = W1_0 - lr * G1\n",
    "print(\"Weights update at First layer:\", W1_1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ee2213",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
